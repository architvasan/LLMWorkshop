{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1tOS7oWba4s"
   },
   "source": [
    "# Large language models (LLMs): Part II\n",
    "\n",
    "Author: Archit Vasan , including materials on LLMs by Varuni Sastri, and discussion/editorial work by Taylor Childers, Carlo Graziani, Bethany Lusch, and Venkat Vishwanath (Argonne)\n",
    "\n",
    "Some inspiration from the blog post \"The Illustrated Transformer\" by Jay Alammar, highly recommended reading.\n",
    "\n",
    "Before you begin, make sure that you have your environment set up and your repo refreshed, as described in previous lessons, and reviewed in the accompanying 'Readme.md' file. Make sure that you select the kernel 'datascience/conda-2023-10-04' at the top-left of the Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief review\n",
    "\n",
    "LLMs are models with a large number of parameters and are able to process sequential data\n",
    "\n",
    "Example: Translation from one language to another (e.g. French ---> English).\n",
    "\n",
    "These models commonly use the Transformer architecture that was introduced in 2017 in the \"Attention is all you need\" paper. Since then a multitude of LLM architectures have been designed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spruqgx1qI06"
   },
   "source": [
    "![en_chapter1_transformers_chrono.svg](images/en_chapter1_transformers_chrono.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7PCbnnt4STj"
   },
   "source": [
    "Image credit: https://huggingface.co/learn/nlp-course/chapter1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are LLMs used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Typically using Huggingface and the transformer library.**\n",
    "\n",
    "Several tools and libraries are available for working with Large Language Models. In this tutorial we will look at the \"transformers\" which is a popular library for natural language understanding and generation tasks, built on top of PyTorch and TensorFlow.\n",
    "\n",
    "HuggingFace is a platform and community that provides open-source library tools and resources like pre-trained models and datasets.\n",
    "\n",
    "Refer to the following links for more information :\n",
    "\n",
    "https://huggingface.co/docs/hub/index\n",
    "https://huggingface.co/docs/transformers/en/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 : Installations and imports\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# STEP 2 : Set up the prompt\n",
    "input_text = \"The panoramic view of the ocean was breathtaking.\"\n",
    "\n",
    "# STEP 3 : Load the pretrained model.\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "#STEP 4 : Load the tokenizer and tokenize the input text\n",
    "tokenizer  =  AutoTokenizer.from_pretrained(model_name)\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# STEP 5 : Perform inference\n",
    "outputs = model(input_ids)\n",
    "result = outputs.logits\n",
    "\n",
    "# STEP 6 :  Interpret the output.\n",
    "probabilities = F.softmax(result, dim=-1)\n",
    "print(probabilities)\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "out_string = \"[{'label': '\" + str(labels[predicted_class]) + \"', 'score': \" + str(probabilities[0][predicted_class].tolist()) + \"}]\"\n",
    "print(out_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussions from Carlo's lesson:\n",
    "   * Different types of sequential data\n",
    "   * Tokenization methods\n",
    "   * Token embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqx2azLTiRLz"
   },
   "source": [
    "Generally, there are 3 types of LLMs I will discuss here:\n",
    "\n",
    "* Encoder-decoder Transformers\n",
    "* Encoder-only Transformers\n",
    "* Decoder-only Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS AND HYPERPARAMETERS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "n_embd = 16 # what is the embedding size of each of the token ?\n",
    "n_head = 4  # Number of parallel heads in the multi head attention\n",
    "n_layer = 4 # Total number of blocks you have in your encoder/decoder\n",
    "vocab_size = 65 # Total number of distinct tokens/words you have in your vocabulary\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmbDaGypbmRE"
   },
   "source": [
    "## Overview of Transformers\n",
    "\n",
    "We will now introduce the \"vanilla\" Transformer architecture introduced in \"Attention is all you need\". This is an encoder and decoder region with connections in between.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkgYJa2BqTO6"
   },
   "source": [
    "![Transformer_Arch.png](images/Transformer_Arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yqx8eMl1gCEV"
   },
   "source": [
    "The encoder/decoder regions are each made of stacked blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ak8G74CqcKU"
   },
   "source": [
    "![Transformer_Enc_Dec_Blocks.png](images/Transformer_Enc_Dec_Blocks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb2jni7yifdE"
   },
   "source": [
    "Each encoder block consists of a self-attention layer connected to a feed-forward layer.   \n",
    "\n",
    "The decoder block also starts with a self-attention layer which is then connected to an encoder-decoder attention layer and followed by a feed-forward layer.\n",
    "\n",
    "I will go into detail on what \"attention\" means later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syVyMFUdqisF"
   },
   "source": [
    "![encode_decode.png](images/encode_decode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPnOYsJHkWc5"
   },
   "source": [
    "### Adding text + tensors into the picture\n",
    "\n",
    "Words are turned into vectors based on their location within a vocabulary.\n",
    "\n",
    "For example with a vocabulary of nine words, each word in the vocabulary can be depicted as a one-hot encoding within this vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(text[0:100])\n",
    "print(\"==============\")\n",
    "print((chars))\n",
    "print(stoi)\n",
    "print(encode('A'))\n",
    "print(decode(encode('A')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQbOa9Dpqo3y"
   },
   "source": [
    "![wordembedding.png](images/wordembedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rp7M5Njs4nil"
   },
   "source": [
    "Image credit: https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6s09HvFhmWyx"
   },
   "source": [
    "Once the words are encoded as vectors then each vector streams through the encoding layers as in the following example for the two words \"Thinking\" and \"Machines\".\n",
    "\n",
    "While each word streams through the model by itself, there are connections within the attention layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFq78-kjbrWp"
   },
   "source": [
    "### Positional encoding\n",
    "\n",
    "Positional encoding accounts for the order of the words in the input sequence.\n",
    "\n",
    "The Transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIgIWpQlqw_M"
   },
   "source": [
    "![transformer_positional_encoding_vectors.png](images/transformer_positional_encoding_vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fxj9hs0dBRHl"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CFUZov-q6kc"
   },
   "source": [
    "![encoder_with_tensors_2.png](images/encoder_with_tensors_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paegfAF27wCp"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "print(token_embedding_table)\n",
    "print(token_embedding_table.weight)\n",
    "x = torch.tensor([1,3,15,4,7,1,4,9])\n",
    "x = token_embedding_table(x) + position_embedding_table(torch.arange(block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
    "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
    "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
    "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
    "targets = torch.tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
    "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
    "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
    "        [17, 27, 10,  0, 21,  1, 54, 39]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BowLYFlCrDrr"
   },
   "source": [
    "### Self-attention mechanisms\n",
    "\n",
    "Now I will explain self-attention at a very high level.\n",
    "\n",
    "Say the following sentence is an input sentence we want to translate:\n",
    "\n",
    "**”The animal didn't cross the street because it was too tired”**\n",
    "\n",
    "When the model processes the word “it”, self-attention associates “it” with “animal”.\n",
    "\n",
    "As the model processes each word in the input sequence, self attention looks at other positions in the input sequence for clues to a better encoding for this word.\n",
    "\n",
    "![transformer_self-attention_visualization.png](images/transformer_self-attention_visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGbAi0cJ7x3a"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jzf9VE_AqWeR"
   },
   "source": [
    "For self-attention there are 5 general steps:\n",
    "\n",
    "1. Generate query, key and value vectors for each word:\n",
    "\n",
    "* These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
    "*  These vectors are abstractions useful for calculating and thinking about attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seNwDWKHU889"
   },
   "source": [
    "2. Calculate a score for each word in the input sentence against each other.\n",
    "\n",
    "* Say we’re calculating the self-attention for the first word in this example, “Thinking”.\n",
    "* We need to score each word of the input sentence against this word.\n",
    "* The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAgbhNesVWd9"
   },
   "source": [
    "3. Divide the scores by the square root of the dimension of the key vectors to stabilize the gradients. This is then passed through a softmax operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flICEgX9VnkD"
   },
   "source": [
    "4. Multiply each value vector by the softmax score.\n",
    "* Want to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvabliwsWT5p"
   },
   "source": [
    "5. Sum up the weighted value vectors.\n",
    "* This produces the output of the self-attention layer at this position for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJwJLVP5rPSe"
   },
   "source": [
    "![self-attention-output.png](images/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOwm-NkXA8U3"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,4,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "print(\"Input\")\n",
    "print(x[0,:,:])\n",
    "print(\"=================================\")\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "print(\"Lower Triangular Matrix\")\n",
    "print(tril)\n",
    "print(\"=================================\")\n",
    "wei = torch.zeros((T,T))\n",
    "print(\"Weights initially\")\n",
    "print(wei)\n",
    "print(\"=================================\")\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "print(\"Masked weights\")\n",
    "print(wei)\n",
    "print(\"=================================\")\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(\"Masked weights after softmax\")\n",
    "print(wei)\n",
    "print(\"=================================\")\n",
    "xbow = wei @ x\n",
    "print(\"Aggregation\")\n",
    "print(xbow.shape)\n",
    "\n",
    "print(\"Output\")\n",
    "print(xbow[0,:,:])\n",
    "print(\"=================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Self Attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Here we want the wei to be data dependent - ie gather info from the past but in a data dependant way\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16) # each token here (totally B*T) produce a key and query in parallel and independently\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T). #\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # this is only present in a decoder block not in an encoder\n",
    "print(wei[0]) # Wei is now not uniform anymore !! It is data dependent\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and normalize giving a nice distibution that sums to 1 and\n",
    "                             # now it tells us that in a data dependent manner how much of info to aggregate from any of the past tokens\n",
    "print(wei[0])\n",
    "v = value(x)\n",
    "out = wei @ v # aggregate the attention scores and value vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lwyFlxKW6oA"
   },
   "source": [
    "### Multi-head attention\n",
    "\n",
    "In practice, multiple attention heads are used which\n",
    "1. Expands the model’s ability to focus on different positions and prevent the attention to be dominated by the word itself.\n",
    "2. Have multiple “representation subspaces”. Have multiple sets of Query/Key/Value weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kRwzpwZrWzH"
   },
   "source": [
    "![transformer_multi-headed_self-attention-recap.png](images/transformer_multi-headed_self-attention-recap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oHsezdVBIaf"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_i9ezLNadC2"
   },
   "source": [
    "The attention mechanisms can be significantly more complex as the number of heads increases!\n",
    "* Note: each color here represents the attention from different attention heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWzxH2h1rfmM"
   },
   "source": [
    "![transformer_self-attention_visualization_3.png](images/transformer_self-attention_visualization_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJMmXEWcBPvL"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAje5iQbrphT"
   },
   "source": [
    "![encoder_with_tensors_2.png](images/encoder_with_tensors_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GeqyRoIQ9Mj"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfBGUyOsd04y"
   },
   "source": [
    "### Adding in decoders\n",
    "\n",
    "Now that we know how the encoder layers work the decoder layers are much more straightforward to understand:\n",
    "\n",
    "The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.\n",
    "\n",
    "In the decoder, the self-attention layer only attends to earlier positions in the output sequence. The future positions are masked (setting them to -inf) before the softmax step in the self-attention calculation.\n",
    "\n",
    "The “Encoder-Decoder Attention” layer creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctEkOn7mryXo"
   },
   "source": [
    "![encode_decode.png](images/encode_decode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "205VVoo4fffm"
   },
   "source": [
    "The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output.\n",
    "\n",
    "The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did.\n",
    "\n",
    "And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnFV9h7osLGW"
   },
   "source": [
    "![transformer_decoding_2.gif](images/transformer_decoding_2.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfcZ_S8MBSvC"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF1HzH9xNJ7S"
   },
   "source": [
    "### How do we turn the output of the decoder stack into a word?\n",
    "\n",
    "Using the final Linear layer and a Softmax Layer.\n",
    "\n",
    "The Linear layer projects the vector produced by the stack of decoders, into a larger vector called a logits vector.\n",
    "\n",
    "If our model knows 10,000 unique English words learned from its training dataset the logits vector is 10,000 cells wide – each cell corresponds to the score of a unique word.\n",
    "\n",
    "The softmax layer turns those scores into probabilities. The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKAlc6FXsit6"
   },
   "source": [
    "![transformer_decoder_output_softmax.png](images/transformer_decoder_output_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS6r-z8dN_RV"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIWj0V3utkt4"
   },
   "source": [
    "### Training\n",
    "\n",
    "To visualize training, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “\\<eos\\>” (short for ‘end of sentence’)).\n",
    "\n",
    "Each word in the vocabulary can be outputted as a one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9WWNO0YszbO"
   },
   "source": [
    "![one-hot-vocabulary-example.png](images/one-hot-vocabulary-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYtyD_Ft4p55"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_I_gqkm3aDQ"
   },
   "source": [
    "Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.\n",
    "\n",
    "We want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7FMBfkls-XL"
   },
   "source": [
    "![transformer_logits_output_and_label.png](images/transformer_logits_output_and_label.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ixVZtHg4qtH"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK8q67P03yr4"
   },
   "source": [
    "To compare these distributions we can simply look at the difference between them a loss like cross-entropy or Kullback–Leibler divergence. Then the training uses back-propagation to optimize this loss function.\n",
    "\n",
    "A more complex situation is translating the sentence: “je suis étudiant” into “i am a student” as can be seen in the example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRsqsi21tI15"
   },
   "source": [
    "![output_target_probability_distributions.png](images/output_target_probability_distributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNMufYf44rYa"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qBLH0DckwxU"
   },
   "source": [
    "### Advantages and disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Sequence-to-sequence tasks: Well-suited for tasks where the input and output sequences have different lengths, such as machine translation or summarization.\n",
    "* Information compression: The encoder compresses input information into a fixed-size context vector, which the decoder then uses to generate the output sequence.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* Computationally expensive: Requires processing the entire input sequence before generating any part of the output sequence, which can be computationally expensive.\n",
    "* Not suitable for autoencoding tasks: May not be the best choice for tasks where the input and output sequences are expected to be similar or identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjLqoEJTtWku"
   },
   "source": [
    "## Tokenization for language models\n",
    "\n",
    "Now we will have discuss different ways that language models recognize and “read” text.\n",
    "\n",
    "Humans do this inherently because they previously learned phonetic sounds. Machines don’t have phonetic knowledge so they need to be told how to break text into standard units to process it.\n",
    "They use a system called “tokenization”, where sequences of text are broken into smaller parts, or “tokens”, and then fed as input.\n",
    "\n",
    "![text-processing---machines-vs-humans.png](images/text-processing---machines-vs-humans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF8jQqv2Iqa_"
   },
   "source": [
    "Image credit: https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYjPS-FptjI0"
   },
   "source": [
    "### Tokenizing based on \"words\"\n",
    "\n",
    "Based on syntax of English language a likely answer is just that breaking sentences into word-level chunks or tokens seems like the best approach.\n",
    "\n",
    "Although this seems easy, it can actually be done in different ways as shown in the following diagram.\n",
    "\n",
    "![tokenize_words.png](images/tokenize_words.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqT-1stEIvJ8"
   },
   "source": [
    "Image credit: https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVKYssvMH7tU"
   },
   "source": [
    "There are some issues with this approach though:\n",
    "\n",
    "* You need a big vocabulary: You can only learn those words in your training vocab. Any words not in the training set will be treated as unknown words. It does not break words into sub-words so it would miss anything like “talk” vs. “talks” vs. “talked” and “talking”.\n",
    "* Words are combined: There may be some confusion about what exactly constitutes a word. Some words such as “sun” and “flower” are compounded to make sunflower. Are these one word or multiple?\n",
    "* Some languages don’t segment by spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5qZKrpvJAlK"
   },
   "source": [
    "### Character-based tokenization\n",
    "\n",
    "To potentially solve this we can try to simply tokenize the input text character by character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_jsrK_mtrJN"
   },
   "source": [
    "![chars-tokenization.png](images/chars-tokenization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ITNMqvdJpPC"
   },
   "source": [
    "Image credit: https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQUK4D67KJKr"
   },
   "source": [
    "Issues with this approach:\n",
    "* Lack of meaning: Unlike words, characters don’t have any inherent meaning, so there is no guarantee that the resultant learned representations will have any meaning.\n",
    "* Increased input computation: If you use word level tokens then you will spike a 7-word sentence into 7 input tokens. However, assuming an average of 5 letters per word (in the English language) you now have 35 inputs to process. This increases the complexity of the scale of the inputs you need to process\n",
    "* Limits network choices: Increasing the size of your input sequences at the character level also limits the type of neural networks you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0HJjUTpt2K7"
   },
   "source": [
    "### Subword tokenization\n",
    "This tokenization type deals with an infinite potential vocabulary via a finite list of known words.\n",
    "\n",
    "There are different ways of doing this:\n",
    "\n",
    "**Byte-pair encoding**\n",
    "\n",
    "![Byte_Pair_enc.webp](images/Byte_Pair_enc.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eojXMQljL9yk"
   },
   "source": [
    "Image credit: https://towardsdatascience.com/tokenization-algorithms-explained-e25d5f4322ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ciy9s6wLWso"
   },
   "source": [
    "BPE was initially introduced to help compress data by finding common byte pair combinations.\n",
    "\n",
    "This tokenization first forms a base vocabulary which is a collection of all unique characters present in the corpus. We also calculate frequency of each token and represent each token as a list of individual characters from base vocabulary.\n",
    "\n",
    "Now merging begins. We keep adding tokens to our base vocab as long as the maximum size is not breached on the basis of following criteria — the pair of tokens occurring most number of times is merged and introduced as a new token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnazXlhpMY7p"
   },
   "source": [
    "**Word-piece tokenizer**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tOHGsXIuBxb"
   },
   "source": [
    "Word-piece tokenization is similar to BPE but instead maximizes the likelihood of token pairs:\n",
    "\n",
    "![WordPieceTok.webp](images/WordPieceTok.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4N8O45PMvJR"
   },
   "source": [
    "Word-piece and BPE will go through every potential option at each step and pick the tokens to merge based on the highest frequency/likelihood. In this way it is a greedy algorithm which optimizes for the best solution at each step in its iteration.\n",
    "\n",
    "However, this greedy algorithm can result in a potentially ambiguous final token vocabulary. This is especially for when there is more than one way to encode a particular word. How do you choose which subword units to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTi462A4RgEK"
   },
   "source": [
    "**Unigram**\n",
    "Unigram sticks to predicting the most likely result token taking into account learned probability during training. How likely it is that the next word is “learning” depends only on the probability of the word “learning” turning up in the training set.\n",
    "\n",
    "To generate a unigram subword token set you need to first define the desired final size of your token set and also a starting seed subword token set.\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Work out the probability for each subword token\n",
    "2. Work out a loss value which would result if each subwork token were to be dropped. The loss is worked out via an algorithm described in the paper (Kudo 2018) (an expectation maximization algorithm).\n",
    "3. Drop the tokens which have the largest loss value. You can choose a value here, e.g. drop the bottom 10% or 20% of subword tokens based on their loss calculations. Note you need to keep single characters to be able to deal with out-of-vocabulary words.\n",
    "4. Repeat these steps until you reach your desired final vocabulary size or until there is no change in token numbers after successive iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJ8TQcY0h1zD"
   },
   "source": [
    "## Encoder-only Transformers\n",
    "\n",
    "In addition to the encoder-decoder architecture shown here there various other architectures which are either only encoder or decoder models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVEqV-ogiKlN"
   },
   "source": [
    "### Bidirectional Encoder Representations from Transformers (BERT) model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykdG_rXojZx6"
   },
   "source": [
    "Encoder-only models only use the encoder layer of the Transformer.\n",
    "\n",
    "These models are usually used for \"understanding\" natural language; however, they typically are not used for text generation. Examples of uses for these models are:\n",
    "\n",
    "1. Determining how positive or negative a movie’s reviews are. (Sentiment Analysis)\n",
    "2. Summarizing long legal contracts. (Summarization)\n",
    "3. Differentiating words that have multiple meanings (like ‘bank’) based on the surrounding text. (Polysemy resolution)\n",
    "\n",
    "These models are often characterized as having “bi-directional” attention, and are often called auto-encoding models.\n",
    "The attention mechanisms of these models can access all the words in the initial sentence.\n",
    "\n",
    "The most common encoder only architectures are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CCuGvS4h9G4"
   },
   "source": [
    "* ALBERT\n",
    "* BERT\n",
    "* DistilBERT\n",
    "* ELECTRA\n",
    "* RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWQ2w9niYhpL"
   },
   "source": [
    "As example, let's consider BERT model in a little more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zqTajCjuMwz"
   },
   "source": [
    "![BERT_Explanation.webp](images/BERT_Explanation.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmOQzlfbbSRt"
   },
   "source": [
    "Image credit: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn4B4vvMlR0a"
   },
   "source": [
    "The BERT model is bidirectionally trained to have a deeper sense of language context and flow than single-direction language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQJ9gfGHmBwv"
   },
   "source": [
    "The Transformer encoder reads the entire sequence of words at once. Therefore it is considered bidirectional. This characteristic allows the model to learn the context of a word based on all of its surroundings (left and right of the word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bGytwTrmZY8"
   },
   "source": [
    "In the BERT training process, the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPlkDoWCnCPS"
   },
   "source": [
    "To help the model distinguish between the two sentences in training, the input is processed in the following way before entering the model:\n",
    "\n",
    "1. A [CLS] token is inserted at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n",
    "2. A sentence embedding indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.\n",
    "3. A positional embedding is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vuT3OI2uUpc"
   },
   "source": [
    "![BERT_input_sent.webp](images/BERT_input_sent.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i0EoRIDbVFE"
   },
   "source": [
    "Image credit: https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsMlclp_nVKV"
   },
   "source": [
    "To predict if the second sentence is indeed connected to the first, the following steps are performed:\n",
    "\n",
    "1. The entire input sequence goes through the Transformer model.\n",
    "2. The output of the [CLS] token is transformed into a 2×1 shaped vector, using a simple classification layer (learned matrices of weights and biases).\n",
    "3. Calculating the probability of IsNextSequence with softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OaH5uh5hOQE"
   },
   "source": [
    "### Advantages and disadvantages:\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "* Contextualized embeddings: Good for tasks where contextualized embeddings of input tokens are crucial, such as natural language understanding.\n",
    "* Parallel processing: Allows for parallel processing of input tokens, making it computationally efficient.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* Not designed for sequence generation: Might not perform well on tasks that require sequential generation of output, as there is no inherent mechanism for auto-regressive decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbSJ7_J9sK8Z"
   },
   "source": [
    "Here is an example of a BERT code that can be used to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Abjp33PHe3Ts"
   },
   "source": [
    "## Decoder-only models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d82IczeVoAuV"
   },
   "source": [
    "### Generative Pre-trained Transformer (GPT)-2\n",
    "\n",
    "Decoder models use only the decoder of a Transformer model. At each stage, for a given word the attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.\n",
    "\n",
    "The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
    "\n",
    "These models are best suited for tasks involving text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4PcUAAhoA2H"
   },
   "source": [
    "Examples of these include:\n",
    "* CTRL\n",
    "* GPT\n",
    "* GPT-2\n",
    "* Transformer XL\n",
    "\n",
    "Let's discuss one of the most popular models, GPT-2 in a little more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdGPhHW9Pm0U"
   },
   "source": [
    "The architecture of GPT-2 is inspired by the paper: \"Generating Wikipedia by Summarizing Long Sequences\" which is another arrangement of the transformer block that can do language modeling. This model threw away the encoder and thus is known as the “Transformer-Decoder”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cig2mvfguetQ"
   },
   "source": [
    "![transformer-decoder-intro.png](images/transformer-decoder-intro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2jNiUvYcZUX"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkIvScEFQSBg"
   },
   "source": [
    "An important difference of the GPT-2 architecture compared to the encoder-Transformer architecture has to do with the type of attention mechanism used.\n",
    "\n",
    "In models such as BERT, the self-attention mechanism has access to tokens to the left and right of the query token. However, in decoder-based models such as GPT-2, masked self-attention is used instead which allows access only to tokens to the left of the query.\n",
    "\n",
    "The masked self-attention mechanism is important for GPT-2 since it allows the model to be trained for token-by-token generation without simply \"memorizing\" the future tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plPvVIOwunQW"
   },
   "source": [
    "![self-attention-and-masked-self-attention.png](images/self-attention-and-masked-self-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwvskuePcdRu"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Na3KlgI0Sae_"
   },
   "source": [
    "The masked self-attention adds understanding of associated words to explain contexts of certain words before passing it through a neural network. It assigns scores to how relevant each word in the segment is, and then adds up the vector representation. This is then passed through the feed-forward network resulting in an output vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJShJx93u18W"
   },
   "source": [
    "![gpt2-self-attention-example-2.png](images/gpt2-self-attention-example-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDkNxx81cd0Q"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUMghdKOStK7"
   },
   "source": [
    "The resulting vector then needs to be converted to an output token. A common method of obtaining this output token is known as top-k.\n",
    "\n",
    "Here, the output vector is multiplied by the token embeddings which results in probabilities for each token in the vocabulary. Then the output token is sampled according to this probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMkMD3jEu92U"
   },
   "source": [
    "![gpt2-output.png](images/gpt2-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1wNJMbJcfYM"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-YdoSNljwpP"
   },
   "source": [
    "### Advantages and disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* Auto-regressive generation: Well-suited for tasks that require sequential generation, as the model can generate one token at a time based on the previous tokens.\n",
    "* Variable-length output: Can handle tasks where the output sequence length is not fixed.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* No direct access to input context: The decoder doesn't directly consider the input context during decoding, which might be a limitation for certain tasks.\n",
    "* Potential for inefficiency: Decoding token by token can be less computationally efficient compared to parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqXMaU4kVpWO"
   },
   "source": [
    "## Additional architectures\n",
    "\n",
    "In addition to text, LLMs have also been applied on other data sources such as images and graphs. Here I will describe two particular architectures:\n",
    "1. Vision Transformers\n",
    "2. Graph Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW_Zl35qWIc5"
   },
   "source": [
    "### Vision Transformers\n",
    "\n",
    "Vision Transformers (ViT) is an architecture that uses self-attention mechanisms to process images.\n",
    "\n",
    "The way this works is:\n",
    "\n",
    "1. Split image into patches (size is fixed)\n",
    "2. Flatten the image patches\n",
    "3. Create lower-dimensional linear embeddings from these flattened image patches and include positional embeddings\n",
    "4. Feed the sequence as an input to a transformer encoder\n",
    "5. Pre-train the ViT model with image labels, which is then fully supervised on a big dataset Fine-tune the downstream dataset for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etRxFXTPvEjr"
   },
   "source": [
    "![vision-transformer-vit.png](images/vision-transformer-vit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfbHrv7mW5jI"
   },
   "source": [
    "Image credit: Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw-WLqVrazU2"
   },
   "source": [
    "### Graph Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYSCHt9SvNUJ"
   },
   "source": [
    "![Graphformer.png](images/Graphformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Vb4LRmBa6as"
   },
   "source": [
    "Image credit: Yang, Junhan, et al. \"GraphFormers: GNN-nested transformers for representation learning on textual graph.\" Advances in Neural Information Processing Systems 34 (2021): 28798-28810."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTy_EjwVa3Sn"
   },
   "source": [
    "References:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0YcMN62bmMU"
   },
   "source": [
    "https://huggingface.co/learn/nlp-course/chapter1/4\n",
    "\n",
    "https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "https://towardsdatascience.com/tokenization-algorithms-explained-e25d5f4322ac\n",
    "\n",
    "https://blog.floydhub.com/tokenization-nlp/\n",
    "\n",
    "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "https://jalammar.github.io/illustrated-gpt2/\n",
    "\n",
    "Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020).\n",
    "\n",
    "Yang, Junhan, et al. \"GraphFormers: GNN-nested transformers for representation learning on textual graph.\" Advances in Neural Information Processing Systems 34 (2021): 28798-28810."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDXLTusqxXHf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
