{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1tOS7oWba4s"
   },
   "source": [
    "# Large language models (LLMs): Theoretical applications\n",
    "\n",
    "LLMs are models that have a large number of parameters and are able to process language in various ways such as language generation and translation from one language to another (e.g. French ---> English).\n",
    "\n",
    "These models commonly use the Transformer architecture that was introduced in 2017 in the \"Attention is all you need\" paper. Since then a multitude of LLM architectures have been designed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr06kaJ9jemX"
   },
   "source": [
    "\n",
    "![THistory](images/en_chapter1_transformers_chrono.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7PCbnnt4STj"
   },
   "source": [
    "Image credit: https://huggingface.co/learn/nlp-course/chapter1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqx2azLTiRLz"
   },
   "source": [
    "Generally, there are 3 types of LLMs I will discuss here:\n",
    "\n",
    "* Encoder-decoder Transformers\n",
    "* Encoder-only Transformers\n",
    "* Decoder-only Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmbDaGypbmRE"
   },
   "source": [
    "## Overview of Transformers\n",
    "\n",
    "We will now introduce the \"vanilla\" Transformer architecture introduced in \"Attention is all you need\". This is an encoder and decoder region with connections in between.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_AWBfeX7Oz0"
   },
   "source": [
    "![Transformer_Arch.png]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yqx8eMl1gCEV"
   },
   "source": [
    "The encoder/decoder regions are each made of stacked blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VGzB_H6_1HY"
   },
   "source": [
    "![Transformer_Enc_Dec_Blocks.png]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb2jni7yifdE"
   },
   "source": [
    "Each encoder block consists of a self-attention layer connected to a feed-forward layer.   \n",
    "\n",
    "The decoder block also starts with a self-attention layer which is then connected to an encoder-decoder attention layer and followed by a feed-forward layer.\n",
    "\n",
    "I will go into detail on what \"attention\" means later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Vb4LRmBa6as"
   },
   "source": [
    "Image credit: Yang, Junhan, et al. \"GraphFormers: GNN-nested transformers for representation learning on textual graph.\" Advances in Neural Information Processing Systems 34 (2021): 28798-28810."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTy_EjwVa3Sn"
   },
   "source": [
    "References:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0YcMN62bmMU"
   },
   "source": [
    "https://huggingface.co/learn/nlp-course/chapter1/4\n",
    "\n",
    "https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/\n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "https://towardsdatascience.com/tokenization-algorithms-explained-e25d5f4322ac\n",
    "\n",
    "https://blog.floydhub.com/tokenization-nlp/\n",
    "\n",
    "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\n",
    "\n",
    "https://jalammar.github.io/illustrated-gpt2/\n",
    "\n",
    "Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020).\n",
    "\n",
    "Yang, Junhan, et al. \"GraphFormers: GNN-nested transformers for representation learning on textual graph.\" Advances in Neural Information Processing Systems 34 (2021): 28798-28810."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDXLTusqxXHf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
